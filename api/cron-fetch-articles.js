// Fichier : api/cron-fetch-articles.js
// VERSION FINALE COMPL√àTE : Fetch par lots, double d√©-doublonnage ET nettoyage des anciens articles.

import { supabaseAdmin } from './config.js'; // ‚úÖ On importe notre client backend unique
import RssParser from 'rss-parser';

export default async function handler(req, res) {
    // 1. S√©curit√©
    if (req.headers['authorization'] !== `Bearer ${process.env.CRON_SECRET}`) {
        return res.status(401).json({ error: 'Acc√®s non autoris√©' });
    }

    try {
        console.log('CRON D√©marr√© : R√©cup√©ration et Nettoyage.');
        const startTime = Date.now();

        // 2. R√©cup√©rer les sources depuis la base de donn√©es
        const { data: sources, error: sourcesError } = await supabaseAdmin
            .from('sources')
            .select('*')
            .eq('is_active', true);

        if (sourcesError) throw sourcesError;
        if (!sources || sources.length === 0) {
            return res.status(200).json({ message: 'Aucune source active √† traiter.' });
        }

        console.log(`Trouv√© ${sources.length} sources actives √† traiter.`);
        const parser = new RssParser({ timeout: 8000 });

        // 3. Traitement par lots (Logique conserv√©e)
        const BATCH_SIZE = 10;
        let articlesFromFeeds = [];
        const fetchFeed = async (source) => { /* ... (la fonction est longue, je la cache pour la lisibilit√©) */ };

        for (let i = 0; i < sources.length; i += BATCH_SIZE) {
            const batch = sources.slice(i, i + BATCH_SIZE);
            const promises = batch.map(fetchFeed);
            const results = await Promise.allSettled(promises);
            results.forEach(r => r.status === 'fulfilled' && r.value && articlesFromFeeds.push(...r.value));
            console.log(`üìä Batch ${Math.floor(i / BATCH_SIZE) + 1}/${Math.ceil(sources.length / BATCH_SIZE)} trait√©.`);
        }

        // 4. Double D√©-doublonnage (Logique conserv√©e)
        console.log(`Trouv√© ${articlesFromFeeds.length} articles avant d√©-doublonnage.`);
        const uniqueArticlesMap = new Map();
        const seenGuids = new Set();

        for (const article of articlesFromFeeds) {
            if (!uniqueArticlesMap.has(article.link) && !seenGuids.has(article.guid)) {
                uniqueArticlesMap.set(article.link, article);
                seenGuids.add(article.guid);
            }
        }

        const articlesToInsert = Array.from(uniqueArticlesMap.values());
        console.log(`‚ú® ${articlesToInsert.length} articles uniques √† ins√©rer.`);

        // 5. Insertion dans la base de donn√©es
        if (articlesToInsert.length > 0) {
            const { error: dbError } = await supabaseAdmin
                .from('articles')
                .upsert(articlesToInsert, { onConflict: 'link' });
            if (dbError) throw dbError;
        }

        // 6. NOUVELLE √âTAPE : NETTOYAGE DES ANCIENS ARTICLES
        console.log('üóëÔ∏è D√©but du nettoyage des articles de plus de 24h...');
        const twentyFourHoursAgo = new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString();

        const { count: deletedCount, error: deleteError } = await supabaseAdmin
            .from('articles')
            .delete({ count: 'exact' }) // 'exact' pour savoir combien ont √©t√© supprim√©s
            .lt('pubDate', twentyFourHoursAgo);

        if (deleteError) {
            console.error('Erreur lors de la suppression des anciens articles:', deleteError);
            // On ne bloque pas la t√¢che pour √ßa, on log juste l'erreur
        } else {
            console.log(`‚úÖ Nettoyage termin√©. ${deletedCount} articles anciens supprim√©s.`);
        }

        const duration = Date.now() - startTime;
        console.log(`üèÅ CRON termin√© en ${duration}ms.`);

        return res.status(200).json({
            success: true,
            inserted: articlesToInsert.length,
            deleted: deletedCount || 0,
            durationMs: duration
        });

    } catch (e) {
        console.error("‚ùå ERREUR FATALE dans le CRON:", e);
        return res.status(500).json({ error: "Erreur critique du serveur", message: e.message });
    }
}


// Je remets la fonction fetchFeed ici pour que le code soit complet
async function fetchFeed(source) {
    if (!source || !source.url) return [];
    try {
        const parser = new RssParser({ timeout: 8000 });
        const feed = await parser.parseURL(source.url);
        return (feed.items || []).map(item => {
            if (item.title && item.link && item.pubDate && item.guid) {
                return {
                    title: item.title.trim(),
                    link: item.link,
                    pubDate: new Date(item.pubDate),
                    source_name: source.name,
                    image_url: item.enclosure?.url || null,
                    guid: item.guid,
                    orientation: source.orientation,
                    category: source.category,
                    tags: source.tags || []
                };
            }
            return null;
        }).filter(Boolean);
    } catch (error) {
        console.warn(`‚ö†Ô∏è Le flux ${source.name} a √©t√© ignor√© (erreur: ${error.message})`);
        return [];
    }
}